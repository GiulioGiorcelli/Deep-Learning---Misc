{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from __future__ import print_function, division\n",
    "from builtins import range\n",
    "# Note: you may need to update your version of future\n",
    "# sudo pip install -U future\n",
    "\n",
    "import numpy as np\n",
    "import theano\n",
    "import theano.tensor as T\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.utils import shuffle\n",
    "from theano.tensor.nnet import conv2d\n",
    "from theano.tensor.signal import downsample\n",
    "\n",
    "from util import getImageData, error_rate, init_weight_and_bias, init_filter\n",
    "from ann_theano import HiddenLayer\n",
    "\n",
    "\n",
    "class ConvPoolLayer(object):\n",
    "    def __init__(self, mi, mo, fw=5, fh=5, poolsz=(2, 2)):\n",
    "        # mi = input feature map size\n",
    "        # mo = output feature map size\n",
    "        sz = (mo, mi, fw, fh)\n",
    "        W0 = init_filter(sz, poolsz)\n",
    "        self.W = theano.shared(W0)\n",
    "        b0 = np.zeros(mo, dtype=np.float32)\n",
    "        self.b = theano.shared(b0)\n",
    "        self.poolsz = poolsz\n",
    "        self.params = [self.W, self.b]\n",
    "\n",
    "    def forward(self, X):\n",
    "        conv_out = conv2d(input=X, filters=self.W)\n",
    "        pooled_out = downsample.max_pool_2d(\n",
    "            input=conv_out,\n",
    "            ds=self.poolsz,\n",
    "            ignore_border=True\n",
    "        )\n",
    "        return T.tanh(pooled_out + self.b.dimshuffle('x', 0, 'x', 'x'))\n",
    "\n",
    "\n",
    "class CNN(object):\n",
    "    def __init__(self, convpool_layer_sizes, hidden_layer_sizes):\n",
    "        self.convpool_layer_sizes = convpool_layer_sizes\n",
    "        self.hidden_layer_sizes = hidden_layer_sizes\n",
    "\n",
    "    def fit(self, X, Y, lr=10e-5, mu=0.99, reg=10e-7, decay=0.99999, eps=10e-3, batch_sz=30, epochs=100, show_fig=True):\n",
    "        lr = np.float32(lr)\n",
    "        mu = np.float32(mu)\n",
    "        reg = np.float32(reg)\n",
    "        decay = np.float32(decay)\n",
    "        eps = np.float32(eps)\n",
    "\n",
    "        # make a validation set\n",
    "        X, Y = shuffle(X, Y)\n",
    "        X = X.astype(np.float32)\n",
    "        Y = Y.astype(np.int32)\n",
    "        Xvalid, Yvalid = X[-1000:], Y[-1000:]\n",
    "        X, Y = X[:-1000], Y[:-1000]\n",
    "\n",
    "        # initialize convpool layers\n",
    "        N, c, width, height = X.shape\n",
    "        mi = c\n",
    "        outw = width\n",
    "        outh = height\n",
    "        self.convpool_layers = []\n",
    "        for mo, fw, fh in self.convpool_layer_sizes:\n",
    "            layer = ConvPoolLayer(mi, mo, fw, fh)\n",
    "            self.convpool_layers.append(layer)\n",
    "            outw = (outw - fw + 1) // 2\n",
    "            outh = (outh - fh + 1) // 2\n",
    "            mi = mo\n",
    "\n",
    "        # initialize mlp layers\n",
    "        K = len(set(Y))\n",
    "        self.hidden_layers = []\n",
    "        M1 = self.convpool_layer_sizes[-1][0]*outw*outh # size must be same as output of last convpool layer\n",
    "        count = 0\n",
    "        for M2 in self.hidden_layer_sizes:\n",
    "            h = HiddenLayer(M1, M2, count)\n",
    "            self.hidden_layers.append(h)\n",
    "            M1 = M2\n",
    "            count += 1\n",
    "\n",
    "        # logistic regression layer\n",
    "        W, b = init_weight_and_bias(M1, K)\n",
    "        self.W = theano.shared(W, 'W_logreg')\n",
    "        self.b = theano.shared(b, 'b_logreg')\n",
    "\n",
    "        # collect params for later use\n",
    "        self.params = [self.W, self.b]\n",
    "        for c in self.convpool_layers:\n",
    "            self.params += c.params\n",
    "        for h in self.hidden_layers:\n",
    "            self.params += h.params\n",
    "\n",
    "        # for momentum\n",
    "        dparams = [theano.shared(np.zeros(p.get_value().shape, dtype=np.float32)) for p in self.params]\n",
    "\n",
    "        # for rmsprop\n",
    "        cache = [theano.shared(np.zeros(p.get_value().shape, dtype=np.float32)) for p in self.params]\n",
    "\n",
    "        # set up theano functions and variables\n",
    "        thX = T.tensor4('X', dtype='float32')\n",
    "        thY = T.ivector('Y')\n",
    "        pY = self.forward(thX)\n",
    "\n",
    "        rcost = reg*T.sum([(p*p).sum() for p in self.params])\n",
    "        cost = -T.mean(T.log(pY[T.arange(thY.shape[0]), thY])) + rcost\n",
    "        prediction = self.th_predict(thX)\n",
    "\n",
    "        cost_predict_op = theano.function(inputs=[thX, thY], outputs=[cost, prediction])\n",
    "\n",
    "        # updates = [\n",
    "        #     (c, decay*c + (np.float32(1)-decay)*T.grad(cost, p)*T.grad(cost, p)) for p, c in zip(self.params, cache)\n",
    "        # ] + [\n",
    "        #     (p, p + mu*dp - lr*T.grad(cost, p)/T.sqrt(c + eps)) for p, c, dp in zip(self.params, cache, dparams)\n",
    "        # ] + [\n",
    "        #     (dp, mu*dp - lr*T.grad(cost, p)/T.sqrt(c + eps)) for p, c, dp in zip(self.params, cache, dparams)\n",
    "        # ]\n",
    "\n",
    "        # momentum only\n",
    "        updates = [\n",
    "            (p, p + mu*dp - lr*T.grad(cost, p)) for p, dp in zip(self.params, dparams)\n",
    "        ] + [\n",
    "            (dp, mu*dp - lr*T.grad(cost, p)) for p, dp in zip(self.params, dparams)\n",
    "        ]\n",
    "\n",
    "        train_op = theano.function(\n",
    "            inputs=[thX, thY],\n",
    "            updates=updates\n",
    "        )\n",
    "\n",
    "        n_batches = N // batch_sz\n",
    "        costs = []\n",
    "        for i in range(epochs):\n",
    "            X, Y = shuffle(X, Y)\n",
    "            for j in range(n_batches):\n",
    "                Xbatch = X[j*batch_sz:(j*batch_sz+batch_sz)]\n",
    "                Ybatch = Y[j*batch_sz:(j*batch_sz+batch_sz)]\n",
    "\n",
    "                train_op(Xbatch, Ybatch)\n",
    "\n",
    "                if j % 20 == 0:\n",
    "                    c, p = cost_predict_op(Xvalid, Yvalid)\n",
    "                    costs.append(c)\n",
    "                    e = error_rate(Yvalid, p)\n",
    "                    print(\"i:\", i, \"j:\", j, \"nb:\", n_batches, \"cost:\", c, \"error rate:\", e)\n",
    "\n",
    "        if show_fig:\n",
    "            plt.plot(costs)\n",
    "            plt.show()\n",
    "\n",
    "    def forward(self, X):\n",
    "        Z = X\n",
    "        for c in self.convpool_layers:\n",
    "            Z = c.forward(Z)\n",
    "        Z = Z.flatten(ndim=2)\n",
    "        for h in self.hidden_layers:\n",
    "            Z = h.forward(Z)\n",
    "        return T.nnet.softmax(Z.dot(self.W) + self.b)\n",
    "\n",
    "    def th_predict(self, X):\n",
    "        pY = self.forward(X)\n",
    "        return T.argmax(pY, axis=1)\n",
    "\n",
    "\n",
    "def main():\n",
    "    X, Y = getImageData()\n",
    "    model = CNN(\n",
    "        convpool_layer_sizes=[(20, 5, 5), (20, 5, 5)],\n",
    "        hidden_layer_sizes=[500, 300],\n",
    "    )\n",
    "    model.fit(X, Y)\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
