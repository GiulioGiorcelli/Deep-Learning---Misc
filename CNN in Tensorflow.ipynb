{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# New concepts and differences from Theano:\n",
    "# - stride is the interval at which to apply the convolution\n",
    "# - unlike previous course, we use constant-size input to the network\n",
    "#   since not doing that caused us to start swapping\n",
    "# - the output after convpool is a different size (8,8) here, (5,5) in Theano\n",
    "\n",
    "# https://deeplearningcourses.com/c/deep-learning-convolutional-neural-networks-theano-tensorflow\n",
    "# https://udemy.com/deep-learning-convolutional-neural-networks-theano-tensorflow\n",
    "\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from datetime import datetime\n",
    "from scipy.signal import convolve2d\n",
    "from scipy.io import loadmat\n",
    "from sklearn.utils import shuffle\n",
    "\n",
    "\n",
    "def y2indicator(y):\n",
    "    N = len(y)\n",
    "    ind = np.zeros((N, 10))\n",
    "    for i in xrange(N):\n",
    "        ind[i, y[i]] = 1\n",
    "    return ind\n",
    "\n",
    "\n",
    "def error_rate(p, t):\n",
    "    return np.mean(p != t)\n",
    "\n",
    "\n",
    "def convpool(X, W, b):\n",
    "    # just assume pool size is (2,2) because we need to augment it with 1s\n",
    "    conv_out = tf.nn.conv2d(X, W, strides=[1, 1, 1, 1], padding='SAME')\n",
    "    conv_out = tf.nn.bias_add(conv_out, b)\n",
    "    pool_out = tf.nn.max_pool(conv_out, ksize=[1, 2, 2, 1], strides=[1, 2, 2, 1], padding='SAME')\n",
    "    return tf.nn.relu(pool_out)\n",
    "\n",
    "\n",
    "def init_filter(shape, poolsz):\n",
    "    w = np.random.randn(*shape) / np.sqrt(np.prod(shape[:-1]) + shape[-1]*np.prod(shape[:-2] / np.prod(poolsz)))\n",
    "    return w.astype(np.float32)\n",
    "\n",
    "\n",
    "def rearrange(X):\n",
    "    # input is (32, 32, 3, N)\n",
    "    # output is (N, 32, 32, 3)\n",
    "    N = X.shape[-1]\n",
    "    out = np.zeros((N, 32, 32, 3), dtype=np.float32)\n",
    "    for i in xrange(N):\n",
    "        for j in xrange(3):\n",
    "            out[i, :, :, j] = X[:, :, j, i]\n",
    "    return out / 255\n",
    "\n",
    "\n",
    "def main():\n",
    "    train = loadmat('../large_files/train_32x32.mat') # N = 73257\n",
    "    test  = loadmat('../large_files/test_32x32.mat') # N = 26032\n",
    "\n",
    "    # Need to scale! don't leave as 0..255\n",
    "    # Y is a N x 1 matrix with values 1..10 (MATLAB indexes by 1)\n",
    "    # So flatten it and make it 0..9\n",
    "    # Also need indicator matrix for cost calculation\n",
    "    Xtrain = rearrange(train['X'])\n",
    "    Ytrain = train['y'].flatten() - 1\n",
    "    print len(Ytrain)\n",
    "    del train\n",
    "    Xtrain, Ytrain = shuffle(Xtrain, Ytrain)\n",
    "    Ytrain_ind = y2indicator(Ytrain)\n",
    "\n",
    "    Xtest  = rearrange(test['X'])\n",
    "    Ytest  = test['y'].flatten() - 1\n",
    "    del test\n",
    "    Ytest_ind  = y2indicator(Ytest)\n",
    "\n",
    "    # gradient descent params\n",
    "    max_iter = 6\n",
    "    print_period = 10\n",
    "    N = Xtrain.shape[0]\n",
    "    batch_sz = 500\n",
    "    n_batches = N / batch_sz\n",
    "\n",
    "    # limit samples since input will always have to be same size\n",
    "    # you could also just do N = N / batch_sz * batch_sz\n",
    "    Xtrain = Xtrain[:73000,]\n",
    "    Ytrain = Ytrain[:73000]\n",
    "    Xtest = Xtest[:26000,]\n",
    "    Ytest = Ytest[:26000]\n",
    "    Ytest_ind = Ytest_ind[:26000,]\n",
    "    # print \"Xtest.shape:\", Xtest.shape\n",
    "    # print \"Ytest.shape:\", Ytest.shape\n",
    "\n",
    "    # initial weights\n",
    "    M = 500\n",
    "    K = 10\n",
    "    poolsz = (2, 2)\n",
    "\n",
    "    W1_shape = (5, 5, 3, 20) # (filter_width, filter_height, num_color_channels, num_feature_maps)\n",
    "    W1_init = init_filter(W1_shape, poolsz)\n",
    "    b1_init = np.zeros(W1_shape[-1], dtype=np.float32) # one bias per output feature map\n",
    "\n",
    "    W2_shape = (5, 5, 20, 50) # (filter_width, filter_height, old_num_feature_maps, num_feature_maps)\n",
    "    W2_init = init_filter(W2_shape, poolsz)\n",
    "    b2_init = np.zeros(W2_shape[-1], dtype=np.float32)\n",
    "\n",
    "    # vanilla ANN weights\n",
    "    W3_init = np.random.randn(W2_shape[-1]*8*8, M) / np.sqrt(W2_shape[-1]*8*8 + M)\n",
    "    b3_init = np.zeros(M, dtype=np.float32)\n",
    "    W4_init = np.random.randn(M, K) / np.sqrt(M + K)\n",
    "    b4_init = np.zeros(K, dtype=np.float32)\n",
    "\n",
    "\n",
    "    # define variables and expressions\n",
    "    # using None as the first shape element takes up too much RAM unfortunately\n",
    "    X = tf.placeholder(tf.float32, shape=(batch_sz, 32, 32, 3), name='X')\n",
    "    T = tf.placeholder(tf.float32, shape=(batch_sz, K), name='T')\n",
    "    W1 = tf.Variable(W1_init.astype(np.float32))\n",
    "    b1 = tf.Variable(b1_init.astype(np.float32))\n",
    "    W2 = tf.Variable(W2_init.astype(np.float32))\n",
    "    b2 = tf.Variable(b2_init.astype(np.float32))\n",
    "    W3 = tf.Variable(W3_init.astype(np.float32))\n",
    "    b3 = tf.Variable(b3_init.astype(np.float32))\n",
    "    W4 = tf.Variable(W4_init.astype(np.float32))\n",
    "    b4 = tf.Variable(b4_init.astype(np.float32))\n",
    "\n",
    "    Z1 = convpool(X, W1, b1)\n",
    "    Z2 = convpool(Z1, W2, b2)\n",
    "    Z2_shape = Z2.get_shape().as_list()\n",
    "    Z2r = tf.reshape(Z2, [Z2_shape[0], np.prod(Z2_shape[1:])])\n",
    "    Z3 = tf.nn.relu( tf.matmul(Z2r, W3) + b3 )\n",
    "    Yish = tf.matmul(Z3, W4) + b4\n",
    "\n",
    "    cost = tf.reduce_sum(\n",
    "        tf.nn.softmax_cross_entropy_with_logits(\n",
    "            logits=Yish,\n",
    "            labels=T\n",
    "        )\n",
    "    )\n",
    "\n",
    "    train_op = tf.train.RMSPropOptimizer(0.0001, decay=0.99, momentum=0.9).minimize(cost)\n",
    "\n",
    "    # we'll use this to calculate the error rate\n",
    "    predict_op = tf.argmax(Yish, 1)\n",
    "\n",
    "    t0 = datetime.now()\n",
    "    LL = []\n",
    "    init = tf.global_variables_initializer()\n",
    "    with tf.Session() as session:\n",
    "        session.run(init)\n",
    "\n",
    "        for i in xrange(max_iter):\n",
    "            for j in xrange(n_batches):\n",
    "                Xbatch = Xtrain[j*batch_sz:(j*batch_sz + batch_sz),]\n",
    "                Ybatch = Ytrain_ind[j*batch_sz:(j*batch_sz + batch_sz),]\n",
    "\n",
    "                if len(Xbatch) == batch_sz:\n",
    "                    session.run(train_op, feed_dict={X: Xbatch, T: Ybatch})\n",
    "                    if j % print_period == 0:\n",
    "                        # due to RAM limitations we need to have a fixed size input\n",
    "                        # so as a result, we have this ugly total cost and prediction computation\n",
    "                        test_cost = 0\n",
    "                        prediction = np.zeros(len(Xtest))\n",
    "                        for k in xrange(len(Xtest) / batch_sz):\n",
    "                            Xtestbatch = Xtest[k*batch_sz:(k*batch_sz + batch_sz),]\n",
    "                            Ytestbatch = Ytest_ind[k*batch_sz:(k*batch_sz + batch_sz),]\n",
    "                            test_cost += session.run(cost, feed_dict={X: Xtestbatch, T: Ytestbatch})\n",
    "                            prediction[k*batch_sz:(k*batch_sz + batch_sz)] = session.run(\n",
    "                                predict_op, feed_dict={X: Xtestbatch})\n",
    "                        err = error_rate(prediction, Ytest)\n",
    "                        print \"Cost / err at iteration i=%d, j=%d: %.3f / %.3f\" % (i, j, test_cost, err)\n",
    "                        LL.append(test_cost)\n",
    "    print \"Elapsed time:\", (datetime.now() - t0)\n",
    "    plt.plot(LL)\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
